{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Data Importing and Helper Function Definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DATA_folder  = '../../Data/'\n",
    "data = np.load(DATA_folder+'train_imgs.npy')\n",
    "lbls = np.load(DATA_folder+'train_lbls.npy')\n",
    "test_data = np.load(DATA_folder+'test_imgs.npy')\n",
    "test_lbls = np.load(DATA_folder+'test_lbls.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Definition of RBF Transformer Object:\n",
    "\n",
    "RBF Transformer is a scikit-learn compatible transformer object that implements:\n",
    "\n",
    "    - fit method       - clusters the digit-separated data, and computes cluster centers and inv. sq. deviations\n",
    "    - transform method - based on obtained centers and deviations it computes the fi values (RBF layer outputs) as \n",
    "                         simple Gaussian functions -> Fi_k(x) = exp{ -sqrt( sum[ (x_i-c_k_i)/(dev_i^2) ] ) } \n",
    "                         \n",
    "## Normalizing each Fi row with row sum -> STABILIZED the Regression a lot!!!\n",
    "\n",
    "## For PCA fit_transform() method needs to be separated to fit() and then transform()!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of Clustering Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Clustering:\n",
    "\n",
    "# from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "import random\n",
    "\n",
    "# clust = AgglomerativeClustering(n_clusters=500, linkage='complete')\n",
    "# cluster_labels = clust.fit_predict(dig_data)\n",
    "\n",
    "# Function for data clustering, and computation of cluster center vectors and inv. sq. deviation vectors\n",
    "def form_clusters(data, n_kmeans, n_agglo, n_random, dig):\n",
    "    \n",
    "    n_km = min(n_kmeans,data.shape[0])\n",
    "    n_ag = min(n_agglo ,data.shape[0])\n",
    "    n_rn = min(n_random,data.shape[0])\n",
    "    \n",
    "    lbls_set = []\n",
    "    \n",
    "    if n_km>0:\n",
    "        kmeans = KMeans(n_clusters=n_km, random_state=0, init='k-means++', algorithm='elkan')\n",
    "        lbls_kmeans = kmeans.fit_predict(data)\n",
    "        lbls_set.append(lbls_kmeans)\n",
    "    \n",
    "    if n_ag>0:\n",
    "        #agglo  = SpectralClustering(n_clusters=n_ag, affinity='nearest_neighbors',eigen_solver='arpack',random_state=456,assign_labels='discretize')\n",
    "        agglo  = SpectralClustering(n_clusters=n_ag, affinity='sigmoid',eigen_solver='arpack',random_state=456,assign_labels='discretize')\n",
    "        lbls_agglo  = agglo.fit_predict(data)\n",
    "        lbls_set.append(lbls_agglo)\n",
    "    \n",
    "    centers     = []\n",
    "    \n",
    "    # Find cluster centers and covar matrix:\n",
    "    for lbls in lbls_set:\n",
    "        for k in range(max(lbls)+1):\n",
    "            cluster = data[lbls==k,:]\n",
    "            centers.append(cluster.mean(axis=0))\n",
    "            \n",
    "        del cluster\n",
    "        \n",
    "    # Add random points as centers:\n",
    "    if n_rn>0:\n",
    "        # seed a new random generator (to get repeatable results for hyperparam tuning)\n",
    "        prng = np.random.RandomState()\n",
    "        prng.seed(dig*n_rn)\n",
    "        smpls = prng.choice(data.shape[0],n_rn,replace=False)\n",
    "        for s in smpls:\n",
    "            centers.append(data[s,:])\n",
    "        \n",
    "    return centers;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones((4,5))\n",
    "a[1,1]=5\n",
    "a.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of RBF (Fi) Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# chunks of 100, (10k smpls, 500 centers) ->  110.8s\n",
    "# chunks of 200, (10k smpls, 500 centers) ->  102.3s\n",
    "# chunks of 250, (10k smpls, 500 centers) ->  115.6s\n",
    "# chunks of 500, (10k smpls, 500 centers) ->  145.1s\n",
    "\n",
    "# Function to compute whole Fi output for given dataset (for all RB centers)\n",
    "def fi_transform(data, all_centers):\n",
    "    # data        - given dataset matrix for which to compute fi values\n",
    "    # center      - list of all center vectors on which to compute fi vals\n",
    "\n",
    "    new_data = np.empty((data.shape[0],len(all_centers)))\n",
    "    st = 0\n",
    "    ch = 200\n",
    "    ns = data.shape[0]\n",
    "    # Process data in chunks of 200 smpls (optimal speed)\n",
    "    while st<ns:\n",
    "        en = min(st+ch,ns)\n",
    "        for k in range(len(all_centers)):\n",
    "            # # SAME DEVIATION for ALL DIMENSIONS (better much better than cluster separate):\n",
    "            new_data[st:en,k] = (np.square(data[st:en] - np.repeat(all_centers[k][np.newaxis,:],en-st,axis=0))).mean(axis=1)\n",
    "        new_data[st:en,:] = np.exp(-np.sqrt(new_data[st:en,:]))\n",
    "        st = en\n",
    "        gc.collect()\n",
    "    #print('\\n',new_data.max())\n",
    "    #new_data = np.exp(-np.sqrt(new_data))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition od Kernel PCA Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time, gc\n",
    "\n",
    "# SKLEARN Compatible Transformer - supports fit method (finding kPCA) and transform method (applying kPCA)\n",
    "class myKernelPCA(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    # Transformer initialization (default to 250 dimensions)\n",
    "    def __init__(self, n_kPCA=250, kernel = 'poly', degree=4, whiten=True, debug=False):\n",
    "        self.n_kPCA    = n_kPCA # number of kernel PCA dimensions to be retained\n",
    "        self.kernel    = kernel # kernel type (rbf, poly, cosine)\n",
    "        self.degree    = degree # poly kernel degree\n",
    "        #self.whiten    = whiten # whiten data at output layer (zero mean, un var)\n",
    "        self.whiten    = whiten and (kernel!='laplacian') and (kernel!='rbf')\n",
    "        self.debug     = debug  # debug flag\n",
    "        self.max_smpls = 250    # max number of samples per digit\n",
    "    \n",
    "    # Clusters each digit and finds cluster centers:\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.kPCA = KernelPCA(n_components=self.n_kPCA,kernel=self.kernel,degree=self.degree,copy_X=False,remove_zero_eig=True,random_state=1389)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('Applying Kernel PCA !!!')\n",
    "            t = time.time()\n",
    "        \n",
    "        # If we have less than max number of smpls - fit regular kPCA\n",
    "        if X.shape[0]<(self.max_smpls*10):\n",
    "            self.kPCA.fit(X)\n",
    "            # If required - whiten the data (for clustering)\n",
    "            if self.whiten:\n",
    "                self.scaler = StandardScaler().fit(self.kPCA.transform(X))\n",
    "        \n",
    "        # Else randomly pick max number of smpls for each digit and fit\n",
    "        else:\n",
    "            # Fix the random seed (for repeatable results)\n",
    "            prng = np.random.RandomState()\n",
    "            prng.seed(654)\n",
    "            # Check how many smps are available per digit\n",
    "            l = []\n",
    "            for dig in range(10):\n",
    "                l.append(min(self.max_smpls,sum(y==dig)))\n",
    "            # Preallocate memory\n",
    "            new_data = np.empty((sum(l),X.shape[1]))\n",
    "            # Fill with randomly drawn smpls\n",
    "            st = 0\n",
    "            for dig in range(10):\n",
    "                en = st+l[dig]\n",
    "                data  = X[y==dig]\n",
    "                smpls = prng.choice(data.shape[0],l[dig],replace=False)\n",
    "                new_data[st:en,] = data[smpls,]\n",
    "                st = en\n",
    "            # Fit KernelPCA to new_data\n",
    "            self.kPCA.fit(new_data)\n",
    "            # If required - whiten the data (for clustering)\n",
    "            if self.whiten:\n",
    "                self.scaler = StandardScaler().fit(self.kPCA.transform(new_data))\n",
    "            # Release old vars, and collect garbage\n",
    "            del data, smpls, new_data\n",
    "            gc.collect()\n",
    "            \n",
    "        if self.debug:\n",
    "            print('Kernel PCA fitted in: ',(time.time()-t))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # Clculate Kernel Components based on obtained model\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        if self.debug:\n",
    "            print('Applying Kernel PCA transformation!!! (data', X.shape,')')\n",
    "            t = time.time()\n",
    "        # Preallocate memory:\n",
    "        new_data = np.empty((X.shape[0],self.kPCA.lambdas_.shape[0]))\n",
    "        st = 0\n",
    "        ch = 200\n",
    "        ns = X.shape[0]\n",
    "        # Transform data in chunks of 200 smpls (memory hungry transformation)\n",
    "        while st<ns:\n",
    "            en = min(st+ch,ns)\n",
    "            new_data[st:en,:] = self.kPCA.transform(X[st:en,:])\n",
    "            st = en\n",
    "        \n",
    "        if self.whiten:\n",
    "            new_data = self.scaler.transform(new_data)\n",
    "                \n",
    "        if self.debug:\n",
    "            print('Kernel PCA transformed in: ',time.time()-t)\n",
    "        \n",
    "        return new_data\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X,y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of RBF Transformer (sklearn compatible object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import time, gc\n",
    "\n",
    "# SKLEARN Compatible Transformer - supports fit method (custering data) and transform method (calculating fi values)\n",
    "class myRBFtransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    # Transformer initialization (default to 50 kMeans clusters)\n",
    "    def __init__(self, n_centers=250, rn_ratio=0.1, cl_ratio=0.5, debug=False):\n",
    "        self.n_centers = n_centers              # number of data centers to be formed per digit\n",
    "        self.rn_ratio  = min(1,max(0,rn_ratio)) # random  ratio (between Random Selection and Clustering)\n",
    "        self.cl_ratio  = min(1,max(0,cl_ratio)) # cluster ratio (between kMeans and Agglomerative)\n",
    "        self.debug     = debug                  # debug flag\n",
    "        if self.debug:\n",
    "            print(self.n_centers,self.cl_ratio)\n",
    "    \n",
    "    # Clusters each digit and finds cluster centers:\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.centers     = [] # list of cluster center vectors\n",
    "        self.num_centers = [] # list of number of centers per digit\n",
    "        \n",
    "        # Calc. num. of clusters per digit based on assigned ratio (ratio*(num of smpls / 10))\n",
    "        n_random   = round(self.rn_ratio*self.n_centers)\n",
    "        n_clusters = self.n_centers - n_random\n",
    "        n_kmeans   = round(self.cl_ratio*n_clusters)\n",
    "        n_agglo    = n_clusters - n_kmeans\n",
    "        \n",
    "        if self.debug:\n",
    "            print('Clustering data ',(n_kmeans,n_agglo))\n",
    "            t = time.time()\n",
    "        \n",
    "        # Cluster the data over each digit\n",
    "        for dig in range(10):\n",
    "            # print('Clustering digit: ',dig)\n",
    "            data = X[y==dig,:]\n",
    "            centers = form_clusters(data, n_kmeans, n_agglo, n_random, dig)\n",
    "            self.centers.extend(centers)\n",
    "            self.num_centers.append(len(centers))\n",
    "            \n",
    "        if self.debug:\n",
    "            print('Clustering time: ',(time.time()-t))\n",
    "        \n",
    "        del data, centers\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # Computes fi values (with Gaussian function) based on obtained data centers\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        # Compute all Fi values:\n",
    "        if self.debug:\n",
    "            print('Calculating all Fi outputs !!! (data', X.shape,')')\n",
    "            t = time.time()\n",
    "        all_fis = fi_transform(X, self.centers)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('Fi calculation time: ',time.time()-t)\n",
    "        \n",
    "        return all_fis\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X,y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Test:\n",
    "# rbf = myRBFtransformer(n_kmeans = 10)\n",
    "# a   = rbf.fit_transform(X=data[:500,:],y=lbls[:500])\n",
    "# a.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Pipelining the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import types\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "\n",
    "n_kPCA    = 250\n",
    "degree    = 3\n",
    "kernel    = 'laplacian'\n",
    "n_centers = 50\n",
    "rn_ratio  = 0.3\n",
    "cl_ratio  = 0.3\n",
    "cl_ratio  = 1.0\n",
    "C         = 1e3\n",
    "tol       = 1e-6\n",
    "debug     = False\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca    = myKernelPCA(kernel=kernel,degree=degree,n_kPCA=n_kPCA,debug=debug)\n",
    "rbf    = myRBFtransformer(n_centers=n_centers,rn_ratio=rn_ratio,cl_ratio=cl_ratio, debug=debug)\n",
    "logreg = LogisticRegression(tol=tol,C=C,random_state=12,solver='liblinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipe = Pipeline(steps=[('scal', scaler), ('pca', pca), ('rbf',rbf), ('logreg',logreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "from dask_searchcv import GridSearchCV\n",
    "\n",
    "# Increasing the centers too much leads to overfitting (e.g. 200 centers - 85%, 100 centers - 95%)\n",
    "\n",
    "Ks  = [200,225,250]\n",
    "RNs = [0.0,0.25,0.5,0.75,1.0] # Random Ratio\n",
    "CLs = [1.0] # Cluster Ratio\n",
    "Cs  = [1e3]\n",
    "\n",
    "kernels = ['laplacian']\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'pca__kernel':    kernels,\n",
    "        'rbf__n_centers': Ks,\n",
    "        'rbf__rn_ratio':  RNs,\n",
    "        'rbf__cl_ratio':  CLs,\n",
    "        'logreg__C':      Cs\n",
    "    },\n",
    "#     {\n",
    "#         'pca__degree': degrees\n",
    "#     }\n",
    "]\n",
    "\n",
    "#         'rbf__n_centers': Ks,\n",
    "#         'rbf__rn_ratio':  RNs,\n",
    "#         'rbf__cl_ratio':  CLs,\n",
    "#         'logreg__C':      Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "import gc\n",
    "\n",
    "test_fold = [-1]*data.shape[0]+[0]*test_data.shape[0]\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "tot_data = np.vstack((data,test_data))\n",
    "tot_lbls = np.hstack((lbls,test_lbls))\n",
    "\n",
    "del data,test_data,lbls,test_lbls\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, cv=ps, n_jobs=-1, param_grid=param_grid, refit=False, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Different Kernels (logreg: tol = 1e-3, C=1e3; 60k trainset):\n",
    "# ------------------\n",
    "# FULL kMEANS:\n",
    "# \n",
    "# laplacian - 94.46%\n",
    "# rbf       - 93.24%\n",
    "# poly_3    - 92.95%\n",
    "# sigmoid   - 91.72%\n",
    "# poly_6    - 90.82%\n",
    "# ------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  8hr 24min  3.3s\n",
      "95.36\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    #grid.fit(data, lbls)\n",
    "    grid.fit(tot_data, tot_lbls)\n",
    "\n",
    "print(max(grid.cv_results_['mean_test_score'])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'logreg__C': [1000.0],\n",
       "  'pca__kernel': ['laplacian'],\n",
       "  'rbf__cl_ratio': [1.0],\n",
       "  'rbf__n_centers': [200, 225, 250],\n",
       "  'rbf__rn_ratio': [0.0, 0.25, 0.5, 0.75, 1.0]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Parameter Grid:\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 200, 'rbf__rn_ratio': 0.0}\n",
      "Score: 95.11 %; Fit time: 4527.0 s; Score Time: 4260.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 200, 'rbf__rn_ratio': 0.25}\n",
      "Score: 95.05 %; Fit time: 4527.0 s; Score Time: 4255.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 200, 'rbf__rn_ratio': 0.5}\n",
      "Score: 95.08 %; Fit time: 4007.0 s; Score Time: 4186.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 200, 'rbf__rn_ratio': 0.75}\n",
      "Score: 94.98 %; Fit time: 4486.0 s; Score Time: 3525.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 200, 'rbf__rn_ratio': 1.0}\n",
      "Score: 95.0 %; Fit time: 4121.0 s; Score Time: 3528.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 225, 'rbf__rn_ratio': 0.0}\n",
      "Score: 95.15 %; Fit time: 4697.0 s; Score Time: 3251.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 225, 'rbf__rn_ratio': 0.25}\n",
      "Score: 95.18 %; Fit time: 4652.0 s; Score Time: 3251.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 225, 'rbf__rn_ratio': 0.5}\n",
      "Score: 95.16 %; Fit time: 3835.0 s; Score Time: 1713.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 225, 'rbf__rn_ratio': 0.75}\n",
      "Score: 95.06 %; Fit time: 3833.0 s; Score Time: 1715.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 225, 'rbf__rn_ratio': 1.0}\n",
      "Score: 95.12 %; Fit time: 3534.0 s; Score Time: 1712.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 250, 'rbf__rn_ratio': 0.0}\n",
      "Score: 95.25 %; Fit time: 5207.0 s; Score Time: 3845.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 250, 'rbf__rn_ratio': 0.25}\n",
      "Score: 95.25 %; Fit time: 5207.0 s; Score Time: 3845.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 250, 'rbf__rn_ratio': 0.5}\n",
      "Score: 95.21 %; Fit time: 5134.0 s; Score Time: 3694.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 250, 'rbf__rn_ratio': 0.75}\n",
      "Score: 95.36 %; Fit time: 5129.0 s; Score Time: 3695.0 s;\n",
      "\n",
      "{'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 250, 'rbf__rn_ratio': 1.0}\n",
      "Score: 95.11 %; Fit time: 4864.0 s; Score Time: 3694.0 s;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('### Parameter Grid:\\n')\n",
    "for k in range(len(grid.cv_results_['params'])):\n",
    "    print(grid.cv_results_['params'][k])\n",
    "    sc = np.round(grid.cv_results_['mean_test_score'][k]*100,2)\n",
    "    ftm = np.round(grid.cv_results_['mean_fit_time'][k])\n",
    "    stm = np.round(grid.cv_results_['mean_score_time'][k])\n",
    "    print('Score:',sc,'%; Fit time:',ftm,'s; Score Time:',stm,'s;\\n')\n",
    "# print('\\n### Mean Fit Times:')\n",
    "# print(np.round(grid.cv_results_['mean_fit_time']))\n",
    "# print('\\n### Mean Score Times')\n",
    "# print(np.round(grid.cv_results_['mean_score_time']))\n",
    "# # print('\\n### Mean Train Score')\n",
    "# # print(np.round(grid.cv_results_['mean_train_score']*100,2))\n",
    "# print('\\n### Mean Test Score')\n",
    "# print(np.round(grid.cv_results_['mean_test_score']*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### TOLERANCE = 1e-6 ### #\n",
    "\n",
    "# ### Grid Search Results:\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 50, 'rbf__rn_ratio': 0.0}\n",
    "# Score: 93.83 %; Fit time: 1217.0 s; Score Time: 74.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 50, 'rbf__rn_ratio': 0.25}\n",
    "# Score: 93.89 %; Fit time: 1196.0 s; Score Time: 72.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 50, 'rbf__rn_ratio': 0.5}\n",
    "# Score: 93.86 %; Fit time: 4081.0 s; Score Time: 518.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 50, 'rbf__rn_ratio': 0.75}\n",
    "# Score: 93.92 %; Fit time: 1563.0 s; Score Time: 159.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 50, 'rbf__rn_ratio': 1.0}\n",
    "# Score: 93.77 %; Fit time: 1899.0 s; Score Time: 339.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 75, 'rbf__rn_ratio': 0.0}\n",
    "# Score: 94.27 %; Fit time: 3825.0 s; Score Time: 256.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 75, 'rbf__rn_ratio': 0.25}\n",
    "# Score: 94.31 %; Fit time: 2324.0 s; Score Time: 189.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 75, 'rbf__rn_ratio': 0.5}\n",
    "# Score: 94.33 %; Fit time: 2219.0 s; Score Time: 205.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 75, 'rbf__rn_ratio': 0.75}\n",
    "# Score: 94.19 %; Fit time: 2143.0 s; Score Time: 135.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 75, 'rbf__rn_ratio': 1.0}\n",
    "# Score: 94.28 %; Fit time: 1312.0 s; Score Time: 62.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 100, 'rbf__rn_ratio': 0.0}\n",
    "# Score: 94.49 %; Fit time: 1980.0 s; Score Time: 100.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 100, 'rbf__rn_ratio': 0.25}\n",
    "# Score: 94.45 %; Fit time: 1934.0 s; Score Time: 98.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 100, 'rbf__rn_ratio': 0.5}\n",
    "# Score: 94.46 %; Fit time: 1842.0 s; Score Time: 94.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 100, 'rbf__rn_ratio': 0.75}\n",
    "# Score: 94.55 %; Fit time: 1797.0 s; Score Time: 91.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 100, 'rbf__rn_ratio': 1.0}\n",
    "# Score: 94.51 %; Fit time: 1577.0 s; Score Time: 96.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 125, 'rbf__rn_ratio': 0.0}\n",
    "# Score: 94.66 %; Fit time: 2510.0 s; Score Time: 1699.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 125, 'rbf__rn_ratio': 0.25}\n",
    "# Score: 94.76 %; Fit time: 2537.0 s; Score Time: 1523.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 125, 'rbf__rn_ratio': 0.5}\n",
    "# Score: 94.69 %; Fit time: 2514.0 s; Score Time: 1524.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 125, 'rbf__rn_ratio': 0.75}\n",
    "# Score: 94.73 %; Fit time: 2195.0 s; Score Time: 1418.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 125, 'rbf__rn_ratio': 1.0}\n",
    "# Score: 94.79 %; Fit time: 9405.0 s; Score Time: 1550.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 150, 'rbf__rn_ratio': 0.0}\n",
    "# Score: 94.89 %; Fit time: 9184.0 s; Score Time: 1656.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 150, 'rbf__rn_ratio': 0.25}\n",
    "# Score: 94.84 %; Fit time: 9107.0 s; Score Time: 1656.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 150, 'rbf__rn_ratio': 0.5}\n",
    "# Score: 94.84 %; Fit time: 4831.0 s; Score Time: 1605.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 150, 'rbf__rn_ratio': 0.75}\n",
    "# Score: 94.82 %; Fit time: 11003.0 s; Score Time: 685.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 150, 'rbf__rn_ratio': 1.0}\n",
    "# Score: 94.81 %; Fit time: 10159.0 s; Score Time: 795.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 175, 'rbf__rn_ratio': 0.0}\n",
    "# Score: 94.99 %; Fit time: 13333.0 s; Score Time: 921.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 175, 'rbf__rn_ratio': 0.25}\n",
    "# Score: 94.96 %; Fit time: 12847.0 s; Score Time: 1175.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 175, 'rbf__rn_ratio': 0.5}\n",
    "# Score: 94.9 %; Fit time: 7480.0 s; Score Time: 1871.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 175, 'rbf__rn_ratio': 0.75}\n",
    "# Score: 94.85 %; Fit time: 7169.0 s; Score Time: 2021.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 175, 'rbf__rn_ratio': 1.0}\n",
    "# Score: 94.98 %; Fit time: 5389.0 s; Score Time: 1080.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 200, 'rbf__rn_ratio': 0.0}\n",
    "# Score: 95.11 %; Fit time: 4527.0 s; Score Time: 4260.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 200, 'rbf__rn_ratio': 0.25}\n",
    "# Score: 95.05 %; Fit time: 4527.0 s; Score Time: 4255.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 200, 'rbf__rn_ratio': 0.5}\n",
    "# Score: 95.08 %; Fit time: 4007.0 s; Score Time: 4186.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 200, 'rbf__rn_ratio': 0.75}\n",
    "# Score: 94.98 %; Fit time: 4486.0 s; Score Time: 3525.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 200, 'rbf__rn_ratio': 1.0}\n",
    "# Score: 95.0 %; Fit time: 4121.0 s; Score Time: 3528.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 225, 'rbf__rn_ratio': 0.0}\n",
    "# Score: 95.15 %; Fit time: 4697.0 s; Score Time: 3251.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 225, 'rbf__rn_ratio': 0.25}\n",
    "# Score: 95.18 %; Fit time: 4652.0 s; Score Time: 3251.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 225, 'rbf__rn_ratio': 0.5}\n",
    "# Score: 95.16 %; Fit time: 3835.0 s; Score Time: 1713.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 225, 'rbf__rn_ratio': 0.75}\n",
    "# Score: 95.06 %; Fit time: 3833.0 s; Score Time: 1715.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 225, 'rbf__rn_ratio': 1.0}\n",
    "# Score: 95.12 %; Fit time: 3534.0 s; Score Time: 1712.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 250, 'rbf__rn_ratio': 0.0}\n",
    "# Score: 95.25 %; Fit time: 5207.0 s; Score Time: 3845.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 250, 'rbf__rn_ratio': 0.25}\n",
    "# Score: 95.25 %; Fit time: 5207.0 s; Score Time: 3845.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 250, 'rbf__rn_ratio': 0.5}\n",
    "# Score: 95.21 %; Fit time: 5134.0 s; Score Time: 3694.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 250, 'rbf__rn_ratio': 0.75}\n",
    "# Score: 95.36 %; Fit time: 5129.0 s; Score Time: 3695.0 s;\n",
    "\n",
    "# {'logreg__C': 1000.0, 'pca__kernel': 'laplacian', 'rbf__cl_ratio': 1.0, 'rbf__n_centers': 250, 'rbf__rn_ratio': 1.0}\n",
    "# Score: 95.11 %; Fit time: 4864.0 s; Score Time: 3694.0 s;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
