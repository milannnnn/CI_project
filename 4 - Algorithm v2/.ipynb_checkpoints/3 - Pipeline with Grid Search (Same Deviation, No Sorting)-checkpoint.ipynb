{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Data Importing and Helper Function Definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DATA_folder  = '../../Data/'\n",
    "data = np.load(DATA_folder+'train_imgs.npy')\n",
    "lbls = np.load(DATA_folder+'train_lbls.npy')\n",
    "test_data = np.load(DATA_folder+'test_imgs.npy')\n",
    "test_lbls = np.load(DATA_folder+'test_lbls.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[lbls==0,:].mean(axis=0).shape\n",
    "st = 50\n",
    "en = 100\n",
    "k  = 0\n",
    "all_centers= [data[51,:]]\n",
    "new_data = (np.square(data[st:en] - np.repeat(all_centers[k][np.newaxis,:],data[st:en].shape[0],axis=0))).mean(axis=1)\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Definition of RBF Transformer Object:\n",
    "\n",
    "RBF Transformer is a scikit-learn compatible transformer object that implements:\n",
    "\n",
    "    - fit method       - clusters the digit-separated data, and computes cluster centers and inv. sq. deviations\n",
    "    - transform method - based on obtained centers and deviations it computes the fi values (RBF layer outputs) as \n",
    "                         simple Gaussian functions -> Fi_k(x) = exp{ -sqrt( sum[ (x_i-c_k_i)/(dev_i^2) ] ) } \n",
    "                         \n",
    "## Normalizing each Fi row with row sum -> STABILIZED the Regression a lot!!!\n",
    "\n",
    "## For PCA fit_transform() method needs to be separated to fit() and then transform()!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of Clustering Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Clustering:\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "# clust = AgglomerativeClustering(n_clusters=500, linkage='complete')\n",
    "# cluster_labels = clust.fit_predict(dig_data)\n",
    "\n",
    "# Function for data clustering, and computation of cluster center vectors and inv. sq. deviation vectors\n",
    "def form_clusters(data, n_kmeans, n_agglo):\n",
    "    \n",
    "    n_km = min(n_kmeans,data.shape[0])\n",
    "    n_ag = min(n_agglo ,data.shape[0])\n",
    "    \n",
    "    lbls_set = []\n",
    "    \n",
    "    if n_km>0:\n",
    "        kmeans = KMeans(n_clusters=n_km, random_state=0, init='k-means++', algorithm='elkan')\n",
    "        lbls_kmeans = kmeans.fit_predict(data)\n",
    "        lbls_set.append(lbls_kmeans)\n",
    "    \n",
    "    if n_ag>0:\n",
    "        agglo  = AgglomerativeClustering(n_clusters=n_ag, linkage='complete')\n",
    "        lbls_agglo  = agglo.fit_predict(data)\n",
    "        lbls_set.append(lbls_agglo)\n",
    "    \n",
    "    centers     = []\n",
    "    \n",
    "    # Find cluster centers and covar matrix:\n",
    "    for lbls in lbls_set:\n",
    "        for k in range(max(lbls)+1):\n",
    "            cluster = data[lbls==k,:]\n",
    "            centers.append(cluster.mean(axis=0))\n",
    "            \n",
    "        del cluster\n",
    "        \n",
    "    return centers;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of RBF (Fi) Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# chunks of 100, (10k smpls, 500 centers) ->  110.8s\n",
    "# chunks of 200, (10k smpls, 500 centers) ->  102.3s\n",
    "# chunks of 250, (10k smpls, 500 centers) ->  115.6s\n",
    "# chunks of 500, (10k smpls, 500 centers) ->  145.1s\n",
    "\n",
    "# Function to compute whole Fi output for given dataset (for all RB centers)\n",
    "def fi_transform(data, all_centers):\n",
    "    # data        - given dataset matrix for which to compute fi values\n",
    "    # center      - list of all center vectors on which to compute fi vals\n",
    "\n",
    "    new_data = np.empty((data.shape[0],len(all_centers)))\n",
    "    st = 0\n",
    "    ch = 200\n",
    "    ns = data.shape[0]\n",
    "    # Process data in chunks of 200 smpls (optimal speed)\n",
    "    while st<ns:\n",
    "        en = min(st+ch,ns)\n",
    "        for k in range(len(all_centers)):\n",
    "            # # SAME DEVIATION for ALL DIMENSIONS (better much better than cluster separate):\n",
    "            new_data[st:en,k] = (np.square(data[st:en] - np.repeat(all_centers[k][np.newaxis,:],data[st:en].shape[0],axis=0))).mean(axis=1)\n",
    "        st = en\n",
    "        gc.collect()\n",
    "    new_data = np.exp(-np.sqrt(new_data))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of RBF Transformer (sklearn compatible object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# SKLEARN Compatible Transformer - supports fit method (custering data) and transform method (calculating fi values)\n",
    "class myRBFtransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    # Transformer initialization (default to 50 kMeans clusters)\n",
    "    def __init__(self, n_clusters=250, cl_ratio=0.5, debug=False):\n",
    "        self.n_clusters  = n_clusters             # number of clusters to be formed per digit\n",
    "        self.cl_ratio    = min(1,max(0,cl_ratio)) # cluster ratio (between kMeans and Agglomerative)\n",
    "        # self.centers     = []                     # list of cluster center vectors\n",
    "        # self.num_centers = []                     # list of number of centers per digit\n",
    "        self.debug       = debug                  # debug flag\n",
    "        if self.debug:\n",
    "            print(self.n_clusters,self.cl_ratio)\n",
    "    \n",
    "    # Clusters each digit and finds cluster centers:\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.centers     = []\n",
    "        self.num_centers = []\n",
    "        \n",
    "        # Calc. num. of clusters per digit based on assigned ratio (ratio*(num of smpls / 10))\n",
    "        n_kmeans = round(self.cl_ratio*self.n_clusters)\n",
    "        n_agglo  = self.n_clusters - n_kmeans\n",
    "        \n",
    "        if self.debug:\n",
    "            print('Clustering data ',(n_kmeans,n_agglo))\n",
    "        \n",
    "        # Cluster the data over each digit\n",
    "        for dig in range(10):\n",
    "            # print('Clustering digit: ',dig)\n",
    "            data = X[y==dig,:]\n",
    "            centers = form_clusters(data, n_kmeans, n_agglo)\n",
    "            self.centers.extend(centers)\n",
    "            self.num_centers.append(len(centers))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # Computes fi values (with Gaussian function) based on obtained cluster centers\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        # Compute all Fi values:\n",
    "        if self.debug:\n",
    "            print('Calculating all Fi outputs !!!')\n",
    "        all_fis = fi_transform(X, self.centers)\n",
    "        \n",
    "        return all_fis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Test:\n",
    "# rbf = myRBFtransformer(n_kmeans = 10)\n",
    "# a   = rbf.fit_transform(X=data[:500,:],y=lbls[:500])\n",
    "# a.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Pipelining the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import types\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "\n",
    "n_pca      = 545 # First dimension to retain 99 % variance (stanford edu recomendation for images)!!!\n",
    "n_clusters = 500\n",
    "cl_ratio   = 0.1\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca    = PCA(n_components=n_pca, whiten=True, random_state=1234)\n",
    "rbf    = myRBFtransformer(n_clusters=n_clusters,cl_ratio=cl_ratio, debug=False)\n",
    "logreg = LogisticRegression(tol=1e-12,C=1e+12,random_state=12,solver='liblinear')\n",
    "\n",
    "############################################################################\n",
    "# For PCA -> fit_transform() method needs to be modified so it calls fit() #\n",
    "#            and transform() methods separately (not at the same time) !!! #\n",
    "#                                                                          #\n",
    "#         -> otherwise it will produce slightly different outputs when     #\n",
    "#            transform() is called next time (comp. to fit_transform())    #\n",
    "#                                                                          #\n",
    "#            WHICH massively messes up with the OUTPUT of FI LAYER !!!!    #\n",
    "############################################################################\n",
    "\n",
    "def new_fit_transform(self, X, y=None, **fit_params):\n",
    "    if y is None:\n",
    "        # return self.fit(X, **fit_params).transform(X)\n",
    "        self.fit(X, **fit_params)\n",
    "        return self.transform(X)\n",
    "    else:\n",
    "        # return self.fit(X, y, **fit_params).transform(X)\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "    \n",
    "pca.fit_transform = types.MethodType(new_fit_transform, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipe = Pipeline(steps=[('scal', scaler), ('pca', pca), ('rbf',rbf), ('logreg',logreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pipe1 = Pipeline(steps=[('scal', scaler), ('pca', pca)])\n",
    "# new_data = pipe1.fit_transform(test_data)\n",
    "# rbf.fit(new_data, test_lbls)\n",
    "# import time\n",
    "# t = time.time()\n",
    "# new_data_2 = rbf.transform(new_data)\n",
    "# print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_data = test_data[:10000]\n",
    "# my_lbls = test_lbls[:10000]\n",
    "# pipe.fit(data,lbls)\n",
    "\n",
    "# print(pipe.score(my_data,my_lbls)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "from dask_searchcv import GridSearchCV\n",
    "\n",
    "Ks = [25, 50, 100]\n",
    "Rs = [0.9, 1] # 1 kMeans, 0 - Agglo\n",
    "Cs = [5e2,7.5e3]\n",
    "\n",
    "fld = 5\n",
    "dt  = 10000\n",
    "param_grid = [\n",
    "    {\n",
    "        'rbf__n_clusters': Ks,\n",
    "        'rbf__cl_ratio':   Rs,\n",
    "        'logreg__C':       Cs\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, cv=fld, n_jobs=-1, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5th Fold on test_data 10k (with    deviations)             - around 74%\n",
    "# 5th Fold on test_data 10k (without deviations)             - around 83.87%\n",
    "# 5th Fold on test_data 10k (without deviations, no sorting) - around 86.14% (takes 2x more time)\n",
    "\n",
    "# # 3rd Fold on test_data 5k:\n",
    "# Ks = [50]; Rs = [0.9]; Ns = [ 5]; Cs = [1e12]; - 77.20 %\n",
    "# Ks = [50]; Rs = [0.5]; Ns = [10]; Cs = [1e08]; - 76.26 %\n",
    "# Ks = [50]; Rs = [0.5]; Ns = [10]; Cs = [1e05]; - 76.50 %\n",
    "# Ks = [50]; Rs = [0.5]; Ns = [10]; Cs = [1e05]; - 76.50 %\n",
    "# Ks = [50]; Rs = [0.5]; Ns = [10]; Cs = [1e04]; - 77.80 %\n",
    "\n",
    "# 5th Fold on test_data 5k:\n",
    "# Ks = [50]; Rs = [0.5]; Ns = [10]; Cs = [7500]; - 86.12 %\n",
    "\n",
    "# # 5th Fold on test_data 10k:\n",
    "# Ks = [ 50]; Rs = [1.0]; Ns = [10]; Cs = [7500]; - 86.85 %\n",
    "# Ks = [250]; Rs = [1.0]; Ns = [10]; Cs = [7500]; - 85.20 %\n",
    "\n",
    "\n",
    "# # 5th Fold on Test Data 10k:\n",
    "# Ks = [25, 50, 100]; Rs = [0.9, 1]; Cs = [5e2,7.5e3]                - 1hr 13min 33.0s\n",
    "# {'logreg__C': 7500.0, 'rbf__cl_ratio': 0.9, 'rbf__n_clusters': 50} - 87.03%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1hr 13min 33.0s\n",
      "87.03\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    #grid.fit(data, lbls)\n",
    "    grid.fit(test_data[:dt], test_lbls[:dt])\n",
    "    \n",
    "print(max(grid.cv_results_['mean_test_score'])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__C': 7500.0, 'rbf__cl_ratio': 0.9, 'rbf__n_clusters': 50}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.03\n"
     ]
    }
   ],
   "source": [
    "print(max(grid.cv_results_['mean_test_score'])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8552  0.8664  0.8681  0.8573  0.8668  0.8689  0.8664  0.8703  0.858\n",
      "  0.8667  0.8685  0.86  ]\n"
     ]
    }
   ],
   "source": [
    "print(grid.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'logreg__C': [500.0, 7500.0],\n",
       "  'rbf__cl_ratio': [0.9, 1],\n",
       "  'rbf__n_clusters': [25, 50, 100]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
