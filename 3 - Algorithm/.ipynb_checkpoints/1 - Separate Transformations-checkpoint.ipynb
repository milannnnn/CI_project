{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define folder paths:\n",
    "DATA_folder  = '../../Data/'\n",
    "CLUSTER_folder = DATA_folder+'Clusters/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def split_seqs(N, max_len):\n",
    "    seqs = []\n",
    "    for k in range(ceil(N/max_len)):\n",
    "        seqs.append(range(k*max_len, min((k+1)*max_len,N)))\n",
    "    return seqs;\n",
    "\n",
    "def split_seqs_by_list(lens):\n",
    "    seqs = []\n",
    "    start = 0\n",
    "    for l in lens:\n",
    "        seqs.append(range(start, start+l))\n",
    "        start = start+l\n",
    "    return seqs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definition:\n",
    "\n",
    "from sklearn.decomposition import KernelPCA, PCA, NMF\n",
    "\n",
    "def find_transform(data):\n",
    "    \n",
    "    # Params:\n",
    "    n_pca  = 50\n",
    "    n_kpca = 50\n",
    "    n_nmf  = 50\n",
    "    \n",
    "    # Models:\n",
    "    pca  = PCA(n_components=n_pca)\n",
    "    kpca = KernelPCA(n_components=n_kpca, kernel='poly', degree = 9, copy_X=False)\n",
    "    nmf  = NMF(n_components=n_nmf, init = 'nndsvd')\n",
    "    \n",
    "    # Fitting:\n",
    "    pca.fit(data)\n",
    "    kpca.fit(data)\n",
    "    nmf.fit(data)\n",
    "    \n",
    "    return [pca, kpca, nmf];\n",
    "\n",
    "def apply_transform(data, transforms):\n",
    "# Apply the transform to the whole dataset at once:\n",
    "    for k in range(len(transforms)):\n",
    "        current_data = transforms[k].transform(data)\n",
    "        if k==0:\n",
    "            new_data = current_data\n",
    "        else:\n",
    "            new_data = np.hstack((new_data,current_data))\n",
    "    return new_data;\n",
    "\n",
    "def apply_transform_chunky(data, transforms):\n",
    "# Chunkify the dataset, and then transform each chunk:\n",
    "    \n",
    "    # Define max chunk size:\n",
    "    max_chunk_size = 2500\n",
    "    \n",
    "    # Count the number of new dimensions (in case we have different dim reductions):\n",
    "    new_dims  = []\n",
    "    for q in range(len(transforms)):\n",
    "        new_dims.append(transforms[q].transform(data[0].reshape(1,-1)).shape[1])\n",
    "    dim_seqs = split_seqs_by_list(new_dims)\n",
    "        \n",
    "    # Split data into sequences:\n",
    "    dat_seqs = split_seqs(data.shape[0], max_chunk_size)\n",
    "    \n",
    "    # Preallocate memory:\n",
    "    new_data = np.zeros((data.shape[0],sum(new_dims)))\n",
    "    \n",
    "    # Iteratre over each transformation and each data chunk:\n",
    "    for k in range(len(transforms)):\n",
    "        for dat_seq in dat_seqs:\n",
    "            st1 = min(dat_seq)\n",
    "            en1 = max(dat_seq)+1\n",
    "            st2 = min(dim_seqs[k])\n",
    "            en2 = max(dim_seqs[k])+1\n",
    "            new_data[st1:en1,st2:en2] = transforms[k].transform(data[dat_seq])\n",
    "    \n",
    "    return new_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transformation:\n",
    "\n",
    "# k = 3\n",
    "# data = np.load(CLUSTER_folder+'train_'+str(k)+'.npy')\n",
    "\n",
    "# tr  = find_transform(data)\n",
    "# # tst = apply_transform_chunky(data, tr)\n",
    "# tst = apply_transform(data, tr)\n",
    "\n",
    "# tst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definition:\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def form_clusters(data):\n",
    "    n_means = 50\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_means, random_state=0, init='k-means++', algorithm='elkan')\n",
    "    lbls_kmeans = kmeans.fit_predict(data)\n",
    "    \n",
    "    lbls_set = [lbls_kmeans]\n",
    "    \n",
    "    centers     = []\n",
    "    covars      = []\n",
    "    inv_covars  = []\n",
    "    \n",
    "    # Find cluster centers and covar matrix:\n",
    "    for lbls in lbls_set:\n",
    "        for k in range(max(lbls)+1):\n",
    "            cluster = data[lbls==k,:]\n",
    "            centers.append(cluster.mean(axis=0))\n",
    "            # Invert with Moore-Penrose (to avoid singularity problems)\n",
    "            covar = np.cov(cluster.T)\n",
    "            covars.append(covar)\n",
    "            inv_covars.append(np.linalg.pinv(covar))\n",
    "    \n",
    "    return centers, covars, inv_covars;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test:\n",
    "# centers, covars, inv_covars = form_clusters(new)\n",
    "# # Check for 0 variance elements (main diag):\n",
    "# for b in range(len(covars)):\n",
    "#     for k in range(covars[0].shape[0]):\n",
    "#         if covars[b][k,k]==0:\n",
    "#             print(b,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# # Improvised Mahalanobis (to deal with singular covar matrices)\n",
    "# # (not needed -> using Moore-Penrose inverse for covar matrix)\n",
    "# def improv_mahalanobis(center, point, covar):\n",
    "    \n",
    "#     # Pick all relevant variables:\n",
    "#     rel_vars = []\n",
    "#     for b in range(covar.shape[0]):\n",
    "#         if covar[b,b]!=0:\n",
    "#             rel_vars.append(b)\n",
    "    \n",
    "#     # If all are relevant just calc regular Mahalanobis\n",
    "#     if len(rel_vars)==covar.shape[0]:\n",
    "#         mah_dist   = mahalanobis(center,point,covar)\n",
    "    \n",
    "#     # Else remove the unrelevant and calc regular Mahalanobis\n",
    "#     else:\n",
    "#         new_center = center[rel_vars]\n",
    "#         new_point  = point[rel_vars]\n",
    "#         new_covar  = covar[np.ix_(rel_vars, rel_vars)]\n",
    "#         mah_dist   = mahalanobis(new_center,new_point,new_covar)\n",
    "        \n",
    "#     return mah_dist;\n",
    "\n",
    "def fi_func(point, center, inv_covar):\n",
    "    return np.exp(-mahalanobis(center, point, inv_covar));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test:\n",
    "# point = centers[0]*0.0\n",
    "# point = centers[15]*1.1\n",
    "# for b in range(len(centers)):\n",
    "# # for b in [15, 33]:\n",
    "#     center     = centers[b]\n",
    "#     inv_covar  = inv_covars[b]\n",
    "#     print(fi_func(point, center, inv_covar))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "all_transforms = []\n",
    "all_centers    = []\n",
    "all_inv_covars = []\n",
    "\n",
    "# Find centers and covar matrices:\n",
    "for dig in range(10):\n",
    "    print(dig)\n",
    "    data = np.load(CLUSTER_folder+'train_'+str(dig)+'.npy')\n",
    "    all_transforms.append(find_transform(data))\n",
    "    new_data = apply_transform(data, all_transforms[dig])\n",
    "    centers, covars, inv_covars = form_clusters(new_data)\n",
    "    all_centers.append(centers)\n",
    "    all_inv_covars.append(inv_covars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_fi(data_points, centers, inv_covars):\n",
    "    output_data = np.zeros((data_points.shape[0],len(centers)))\n",
    "    for k in range(data_points.shape[0]):\n",
    "        for j in range(len(centers)):\n",
    "            output_data[k,j] = fi_func(data_points[k], centers[j], inv_covars[j])\n",
    "    return output_data;\n",
    "\n",
    "def find_fi_vals(data, all_transforms, all_centers, all_inv_covars):\n",
    "    \n",
    "    # Count the num of centers for each digit:\n",
    "    cnts = []\n",
    "    for k in range(10):\n",
    "        cnts.append(len(all_centers[k]))\n",
    "    # Form sequences (for positioning in final matrix):\n",
    "    cnt_seqs = split_seqs_by_list(cnts)\n",
    "    \n",
    "    # Initialize the fis matrix:\n",
    "    fi_vals = np.zeros((data.shape[0],sum(cnts)))\n",
    "    \n",
    "    # Iterate over each digit:\n",
    "    for dig in range(10):\n",
    "        \n",
    "        print(dig)\n",
    "        \n",
    "        new_data = apply_transform_chunky(data, all_transforms[dig])\n",
    "        \n",
    "        st = min(cnt_seqs[dig])\n",
    "        en = max(cnt_seqs[dig])+1\n",
    "        fi_vals[:,st:en] = apply_fi(new_data, all_centers[dig], all_inv_covars[dig])\n",
    "            \n",
    "    del new_data\n",
    "    return fi_vals;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(CLUSTER_folder+'train_imgs.npy')\n",
    "lbls = np.load(CLUSTER_folder+'train_lbls.npy')\n",
    "fis = find_fi_vals(data, all_transforms, all_centers, all_inv_covars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# logistic = LogisticRegression(multi_class='multinomial',solver='saga')\n",
    "logistic = LogisticRegression(C=1e5)\n",
    "\n",
    "logistic.fit(fis, lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load(CLUSTER_folder+'test_imgs.npy')\n",
    "test_lbls = np.load(CLUSTER_folder+'test_lbls.npy')\n",
    "\n",
    "# test_fis = find_fi_vals(test_data, all_transforms, all_centers, all_inv_covars)\n",
    "\n",
    "predicted_lbls = logistic.predict(test_fis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.64999999999999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 500)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = 0;\n",
    "for k in range(len(test_lbls)):\n",
    "    if test_lbls[k]!=predicted_lbls[k]:\n",
    "        err+=1\n",
    "err = err/len(test_lbls)*100\n",
    "print(err)\n",
    "test_fis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2.14421486174e-24\n",
      "3.63477136309e-06\n",
      "3.82947430071e-22\n",
      "1.2640511999e-14\n",
      "1.45996268538e-25\n",
      "3.48319278549e-13\n",
      "1.6680075877e-20\n",
      "4.84123457764e-15\n",
      "2.08212600581e-10\n",
      "2.27856183142e-19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  5.20547784e-06,   5.38906749e-06,   7.99136780e-05,\n",
       "         1.04999050e-05,   5.20547784e-06])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_lbls[5])\n",
    "for k in range(10):\n",
    "    print(max(test_fis[5][(k*50):(k*50+50)]))\n",
    "    \n",
    "fx = fis[0:5]\n",
    "\n",
    "fx.max(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_fis(fis):\n",
    "    \n",
    "    d = round(fis.shape[1]/10)\n",
    "    \n",
    "    red_fis = np.zeros((fis.shape[0],10)\n",
    "    for k in range(10):\n",
    "       red_fis[:,k] = fis[:,(k*d):(k*d+d)].max(axis = 1)\n",
    "                       \n",
    "    return red_fis;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
