{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Data Importing and Helper Function Definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DATA_folder  = '../../Data/'\n",
    "data = np.load(DATA_folder+'train_imgs.npy')\n",
    "lbls = np.load(DATA_folder+'train_lbls.npy')\n",
    "test_data = np.load(DATA_folder+'test_imgs.npy')\n",
    "test_lbls = np.load(DATA_folder+'test_lbls.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5923, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[lbls==0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Definition of RBF Transformer Object:\n",
    "\n",
    "RBF Transformer is a scikit-learn compatible transformer object that implements:\n",
    "\n",
    "    - fit method       - clusters the digit-separated data, and computes cluster centers and inv. sq. deviations\n",
    "    - transform method - based on obtained centers and deviations it computes the fi values (RBF layer outputs) as \n",
    "                         simple Gaussian functions -> Fi_k(x) = exp{ -sqrt( sum[ (x_i-c_k_i)/(dev_i^2) ] ) } \n",
    "                         \n",
    "## Normalizing each Fi row with row sum -> STABILIZED the Regression a lot!!!\n",
    "\n",
    "## For PCA fit_transform() method needs to be separated to fit() and then transform()!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of Clustering Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Clustering:\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "# clust = AgglomerativeClustering(n_clusters=500, linkage='complete')\n",
    "# cluster_labels = clust.fit_predict(dig_data)\n",
    "\n",
    "# Function for data clustering, and computation of cluster center vectors and inv. sq. deviation vectors\n",
    "def form_clusters(data, n_kmeans, n_agglo):\n",
    "    \n",
    "    n_km = min(n_kmeans,data.shape[0])\n",
    "    n_ag = min(n_agglo ,data.shape[0])\n",
    "    \n",
    "    lbls_set = []\n",
    "    \n",
    "    if n_km>0:\n",
    "        kmeans = KMeans(n_clusters=n_km, random_state=0, init='k-means++', algorithm='elkan')\n",
    "        lbls_kmeans = kmeans.fit_predict(data)\n",
    "        lbls_set.append(lbls_kmeans)\n",
    "    \n",
    "    if n_ag>0:\n",
    "        agglo  = AgglomerativeClustering(n_clusters=n_ag, linkage='complete')\n",
    "        lbls_agglo  = agglo.fit_predict(data)\n",
    "        lbls_set.append(lbls_agglo)\n",
    "    \n",
    "    centers     = []\n",
    "    inv_sq_devs = []\n",
    "    \n",
    "    # Find cluster centers and covar matrix:\n",
    "    for lbls in lbls_set:\n",
    "        for k in range(max(lbls)+1):\n",
    "            \n",
    "            cluster = data[lbls==k,:]\n",
    "            centers.append(cluster.mean(axis=0))\n",
    "            dev = np.std(cluster,axis=0)\n",
    "            dev[dev==0]+=1e-3 # to avoid Infs and NaNs\n",
    "            inv_sq_devs.append(np.reciprocal(np.square(dev)))\n",
    "            \n",
    "        del cluster, dev\n",
    "    return centers, inv_sq_devs;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of RBF (Fi) Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Function to compute whole Fi output for given dataset (for all RB centers)\n",
    "def fi_transform(data, all_centers, all_inv_sq_devs, kPCAs):\n",
    "    # data        - given dataset matrix for which to compute fi values\n",
    "    # center      - list of all center vectors on which to compute fi vals\n",
    "    # inv_sq_devs - list of all reciprocal sq. deviation vectors on which to compute fi vals     \n",
    "\n",
    "    dim1 = 0\n",
    "    for cnt in all_centers:\n",
    "        dim1+=len(cnt)\n",
    "    \n",
    "    new_data = np.empty((data.shape[0],dim1))\n",
    "    st = 0\n",
    "    ch = 200\n",
    "    ns = data.shape[0]\n",
    "    # Process data in chunks of 200 smpls (optimal speed)\n",
    "    while st<ns:\n",
    "        q = 0\n",
    "        en = min(st+ch,ns)\n",
    "        for d in range(10):\n",
    "            trns_data = kPCAs[d].transform(data[st:en])\n",
    "            for k in range(len(all_centers[d])):\n",
    "                new_data[st:en,q] = np.dot(np.square(trns_data - np.repeat(all_centers[d][k][np.newaxis,:],trns_data.shape[0],axis=0)),all_inv_sq_devs[d][k])\n",
    "                q+=1\n",
    "        st = en\n",
    "    new_data = np.exp(-np.sqrt(new_data))\n",
    "    del trns_data; gc.collect()\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of RBF Transformer (sklearn compatible object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import KernelPCA\n",
    "import gc\n",
    "\n",
    "# SKLEARN Compatible Transformer - supports fit method (custering data) and transform method (calculating fi values)\n",
    "class myRBFtransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    # Transformer initialization (default to 50 kMeans clusters)\n",
    "    def __init__(self, n_clusters=250, cl_ratio=0.5, n_fi_max=10, n_kPCA=50, debug=False):\n",
    "        self.n_clusters  = n_clusters             # number of clusters to be formed per digit\n",
    "        self.cl_ratio    = min(1,max(0,cl_ratio)) # cluster ratio (between kMeans and Agglomerative)\n",
    "        self.n_fi_max    = n_fi_max               # num of max fi vals to pick per digit\n",
    "        self.n_kPCA      = n_kPCA                 # number of kernel PCA components\n",
    "        self.debug       = debug                  # debug flag\n",
    "\n",
    "        if self.debug:\n",
    "            print(self.n_clusters,self.cl_ratio,self.n_fi_max)\n",
    "    \n",
    "    # Clusters each digit and finds cluster centers and deviations:\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.kPCAs       = []\n",
    "        self.num_centers = []\n",
    "        self.centers     = []\n",
    "        self.inv_sq_devs = []\n",
    "        \n",
    "        # Calc. num. of clusters per digit based on assigned ratio (ratio*(num of smpls / 10))\n",
    "        n_kmeans = round(self.cl_ratio*self.n_clusters)\n",
    "        n_agglo  = self.n_clusters - n_kmeans\n",
    "        \n",
    "        if self.debug:\n",
    "            print('Clustering data ',(n_kmeans,n_agglo))\n",
    "        \n",
    "        # Cluster the data over each digit\n",
    "        for dig in range(10):\n",
    "            # Select corresponding digit:\n",
    "            data = X[y==dig,:]\n",
    "            # Fit the kernel PCA and transform data:\n",
    "            kPCA = KernelPCA(n_components=self.n_kPCA,kernel='rbf',copy_X=False,remove_zero_eig=True,random_state=111).fit(data)\n",
    "            trns_data = kPCA.transform(data)\n",
    "            # Cluster the data and find centers / devs:\n",
    "            centers, inv_sq_devs = form_clusters(trns_data, n_kmeans, n_agglo)\n",
    "            # Update self:\n",
    "            self.kPCAs.append(kPCA)\n",
    "            self.centers.append(centers)\n",
    "            self.inv_sq_devs.append(inv_sq_devs)\n",
    "            self.num_centers.append(len(centers))\n",
    "        \n",
    "        del centers, inv_sq_devs, data, trns_data\n",
    "        gc.collect()\n",
    "        \n",
    "        # Limit the n_fi_max to the min. number of centers per digit \n",
    "        self.n_fi_max = min(self.n_fi_max,min(self.num_centers))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # Computes fi values (with Gaussian function) based on obtained cluster centers and deviations\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        # Compute all Fi values:\n",
    "        if self.debug:\n",
    "            print('Calculating all Fi outputs !!!')\n",
    "        all_fis = fi_transform(X, self.centers, self.inv_sq_devs, self.kPCAs)\n",
    "        \n",
    "        # Find n max fi vals over each digit:\n",
    "        n_max  = self.n_fi_max\n",
    "        result = np.empty((X.shape[0],n_max*10))\n",
    "        start  = 0\n",
    "        for k in range(10):\n",
    "            end = start + self.num_centers[k]\n",
    "            result[:,(k*n_max):((k+1)*n_max)] = np.sort(all_fis[:,start:end],axis=1)[:,-n_max:]\n",
    "            start = end\n",
    "        del all_fis\n",
    "        \n",
    "        # Normalize each row with the sum of that row's elements:\n",
    "        # result = np.divide(result,np.sum(result,axis=1).reshape((result.shape[0],1)))\n",
    "        row_sum = np.sum(result,axis=1).reshape((result.shape[0],1))\n",
    "        row_sum[row_sum==0]=1e-12\n",
    "        result = np.divide(result,row_sum)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('Transform finished !!!')\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Test:\n",
    "# rbf = myRBFtransformer(n_kmeans = 10)\n",
    "# a   = rbf.fit_transform(X=data[:500,:],y=lbls[:500])\n",
    "# a.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Pipelining the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "\n",
    "n_clusters = 500\n",
    "cl_ratio   = 0.5\n",
    "n_fi_max   = 10\n",
    "n_kPCA     = 200\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rbf    = myRBFtransformer(n_clusters=n_clusters,cl_ratio=cl_ratio,n_fi_max=n_fi_max,n_kPCA=n_kPCA)\n",
    "logreg = LogisticRegression(tol=1e-12,C=1e+12,random_state=12,solver='liblinear')\n",
    "\n",
    "pipe = Pipeline(steps=[('scal', scaler), ('rbf',rbf), ('logreg',logreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# t = time.time()\n",
    "# pipe.fit(test_data[:],test_lbls[:])\n",
    "# print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = time.time()\n",
    "# y = pipe.predict(test_data)\n",
    "# print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum(test_lbls==y)/len(y)*100)\n",
    "\n",
    "# # for k in range(20,50):\n",
    "# #     print(test_lbls[k],y[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "from dask_searchcv import GridSearchCV\n",
    "\n",
    "# Number of Kernel PCA components per digit:\n",
    "Ps = [ 50, 100, 200]\n",
    "# Number of Clusters per digit:\n",
    "Ks = [100, 300, 500]\n",
    "# Ratio between kMeans and Agglomerative:\n",
    "Rs = [0.1, 0.5, 0.9]\n",
    "# Number of Max Fi vals to be analyzed:\n",
    "Ns = [10]\n",
    "# Logreg Regularization constant:\n",
    "Cs = [1e5, 1e12, 1e15]\n",
    "\n",
    "# # BEST SOL = 77.65% at 3 fold -> P = 200, K= 200 (liner kPCA kernel)\n",
    "Ps = [100,200]\n",
    "Ks = [ 50,400]\n",
    "Rs = [0.0,1.0]\n",
    "Ns = [15]\n",
    "Cs = [1e12]\n",
    "\n",
    "# Ps = [150]\n",
    "# Ks = [500]\n",
    "# Rs = [0.5]\n",
    "# Ns = [10]\n",
    "# Cs = [1e12]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'rbf__n_kPCA':     Ps,\n",
    "        'rbf__n_clusters': Ks,\n",
    "        'rbf__cl_ratio':   Rs,\n",
    "        'rbf__n_fi_max':   Ns,\n",
    "        'logreg__C':       Cs\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, cv=4, n_jobs=-1, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#########                               ] | 23% Completed |  4hr  1min 43.8s\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bd14c80e9895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# grid.fit(data, lbls)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask_searchcv/model_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_deprecated_train_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     results = get_async(pool.apply_async, len(pool._pool), dsk, result,\n\u001b[1;32m     74\u001b[0m                         \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_thread_get_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                         pack_exception=pack_exception, **kwargs)\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Cleanup pools associated to dead threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                         \u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Re-execute locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                         \u001b[0mraise_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cache'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/compatibility.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/local.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/local.py\u001b[0m in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0margs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mishashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask_searchcv/methods.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(est, X, y, error_score, fields, params, fit_params)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-572a9d1ba96d>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calculating all Fi outputs !!!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mall_fis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfi_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_sq_devs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkPCAs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Find n max fi vals over each digit:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-97ceb6663f79>\u001b[0m in \u001b[0;36mfi_transform\u001b[0;34m(data, all_centers, all_inv_sq_devs, kPCAs)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mq\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtrns_data\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    # grid.fit(data, lbls)\n",
    "    grid.fit(data[:], lbls[:])\n",
    "    \n",
    "print(max(grid.cv_results_['mean_test_score'])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(max(grid.cv_results_['mean_test_score'])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(grid.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_data = test_data[:7500]\n",
    "tr_lbls = test_lbls[:7500]\n",
    "ts_data = test_data[7500:]\n",
    "ts_lbls = test_lbls[7500:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "\n",
    "n_clusters = 500\n",
    "cl_ratio   = 0.5\n",
    "n_fi_max   = 10\n",
    "n_kPCA     = 150\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rbf    = myRBFtransformer(n_clusters=n_clusters,cl_ratio=cl_ratio,n_fi_max=n_fi_max,n_kPCA=n_kPCA)\n",
    "logreg = LogisticRegression(tol=1e-12,C=1e+12,random_state=12,solver='liblinear')\n",
    "\n",
    "pipe1 = Pipeline(steps=[('scal', scaler), ('rbf',rbf)])\n",
    "pipe2 = Pipeline(steps=[('logreg',logreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1.fit(test_data,test_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_data_1 = pipe1.transform(tr_data)\n",
    "ts_data_1 = pipe1.transform(ts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2.fit(tr_data_1,tr_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe2.score(tr_data_1,tr_lbls),pipe2.score(ts_data_1,ts_lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = pipe1.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_searchcv import GridSearchCV\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "n_clusters = 500\n",
    "cl_ratio   = 0.5\n",
    "n_fi_max   = 10\n",
    "n_kPCA     = 150\n",
    "\n",
    "# 4th Fold on Test Set -> 98.05 with C = 1e13\n",
    "Cs = [1e9,1e10,1e12,1e13]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'logreg__C':       Cs\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe2, cv=4, n_jobs=-1, param_grid=param_grid)\n",
    "\n",
    "with ProgressBar():\n",
    "    # grid.fit(data, lbls)\n",
    "    grid.fit(new_data[:], test_lbls[:])\n",
    "print(max(grid.cv_results_['mean_test_score'])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10000000000000/1e13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
